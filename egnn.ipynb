{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f6a5090",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Simple Impementation of E(n) Equivariant Graph Neural Networks\n",
    "\n",
    "Original paper https://arxiv.org/pdf/2102.09844.pdf by Victor Garcia Satorras, Emiel Hoogeboom, Max Welling"
   ],
   "id": "6f6a5090"
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install torch --pre --extra-index-url https://download.pytorch.org/whl/nightly/cu116\n",
    "# Установка PyTorch с поддержкой CUDA 11.6"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f-G3PhQLwGX",
    "outputId": "ebc0169d-9aca-4f39-f133-d654c214399e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "2f-G3PhQLwGX",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cu116\n",
      "Requirement already satisfied: torch in c:\\users\\kulev\\anaconda3\\lib\\site-packages (1.13.0.dev20220720+cu116)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kulev\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4bU4ixrOJCg1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "4bU4ixrOJCg1"
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xCcBVxnngBu",
    "outputId": "f3ddaf66-e142-4735-e736-2dd171762ae1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "9xCcBVxnngBu",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cb08a10",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load QM9 Dataset"
   ],
   "id": "8cb08a10"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae30de9d",
    "outputId": "433bdfc6-2d33-44dc-9999-f54547f32c43",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "C:\\Users\\kulev\\simple-equivariant-gnn\\simple-equivariant-gnn\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Cloning into 'simple-equivariant-gnn'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/senya-ashukha/simple-equivariant-gnn.git\n",
    "%cd simple-equivariant-gnn"
   ],
   "id": "ae30de9d"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "859f981c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# QM9 is a dataset for Molecular Property Predictions http://quantum-machine.org/datasets/\n",
    "# We will predict Highest occupied molecular orbital energy \n",
    "# https://en.wikipedia.org/wiki/HOMO_and_LUMO\n",
    "# We use data loaders from the official repo\n",
    "\n",
    "from qm9.data_utils import get_data, BatchGraph\n",
    "train_loader, val_loader, test_loader, charge_scale = get_data(num_workers=0)"
   ],
   "id": "859f981c"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05e20004",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Graph Representation"
   ],
   "id": "05e20004"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "d0acbcc0",
    "outputId": "1438ff98-cf95-457f-a1e6-2f3bd97ddd5e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m batch \u001B[38;5;241m=\u001B[39m BatchGraph(\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnext\u001B[49m(), \u001B[38;5;28;01mFalse\u001B[39;00m, charge_scale)\n\u001B[0;32m      2\u001B[0m batch\n",
      "\u001B[1;31mAttributeError\u001B[0m: '_SingleProcessDataLoaderIter' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "batch = BatchGraph(iter(train_loader).next(), False, charge_scale)\n",
    "batch"
   ],
   "id": "d0acbcc0"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "784c0726",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Define Equivariant Graph Convs  & GNN"
   ],
   "id": "784c0726"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "76e5e05f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def index_sum(agg_size, source, idx, cuda):\n",
    "    \"\"\"\n",
    "        source is N x hid_dim [float]\n",
    "        idx    is N           [int]\n",
    "        \n",
    "        Sums the rows source[.] with the same idx[.];\n",
    "    \"\"\"\n",
    "    tmp = torch.zeros((agg_size, source.shape[1]))\n",
    "    tmp = tmp.cuda() if cuda else tmp\n",
    "    res = torch.index_add(tmp, 0, idx, source)\n",
    "    return res"
   ],
   "id": "76e5e05f"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "4d5d55db",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ConvEGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, cuda=True):\n",
    "        super().__init__()\n",
    "        self.hid_dim=hid_dim\n",
    "        self.cuda = cuda\n",
    "        \n",
    "        self.f_e = nn.Sequential(nn.Linear(2 * in_dim + 1, hid_dim), nn.ReLU(), nn.Linear(hid_dim, hid_dim), nn.ReLU())\n",
    "        \n",
    "        self.f_inf = nn.Sequential(nn.Linear(hid_dim, 1), nn.Sigmoid()) \n",
    "        \n",
    "        self.f_h = nn.Sequential(nn.Linear(hid_dim + in_dim, hid_dim), nn.SiLU(), nn.Linear(hid_dim, hid_dim))\n",
    "    \n",
    "    def forward(self, b):\n",
    "        # compute distances for all edges\n",
    "        e_st, e_end = b.edges[:,0], b.edges[:,1]\n",
    "        dists = torch.norm(abs(b.x[e_st] - b.x[e_end]), dim=1).reshape(-1, 1)\n",
    "        \n",
    "        # compute messages\n",
    "        tmp = torch.hstack([b.h[e_st], b.h[e_end], dists])\n",
    "        m_ij = self.f_e(tmp)\n",
    "        \n",
    "        # predict edges\n",
    "        e_ij = self.f_inf(m_ij)\n",
    "        \n",
    "        # average e_ij-weighted messages  \n",
    "        # m_i is num_nodes x hid_dim\n",
    "        m_i = index_sum(b.h.shape[0], e_ij*m_ij, b.edges[:,0], self.cuda)\n",
    "        \n",
    "        # update hidden representations\n",
    "        tmp = torch.hstack([m_i, b.h])\n",
    "        b.h += self.f_h(tmp)\n",
    "\n",
    "        return b"
   ],
   "id": "4d5d55db"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "10aad7c4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NetEGNN(nn.Module):\n",
    "    def __init__(self, in_dim=15, hid_dim=128, out_dim=1, n_layers=7, cuda=True):\n",
    "        super().__init__()\n",
    "        self.hid_dim=hid_dim\n",
    "        \n",
    "        self.emb = nn.Linear(in_dim, hid_dim) \n",
    "\n",
    "        # Make gnn of n_layers\n",
    "        self.gnn = nn.Sequential(*[ConvEGNN(hid_dim, hid_dim, cuda=cuda) for _ in range(n_layers)])\n",
    "      \n",
    "        self.pre_mlp = nn.Sequential(\n",
    "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
    "            nn.Linear(hid_dim, hid_dim))\n",
    "        \n",
    "        self.post_mlp = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
    "            nn.Linear(hid_dim, out_dim))\n",
    "\n",
    "        if cuda: self.cuda()\n",
    "        self.cuda = cuda\n",
    "    \n",
    "    def forward(self, b):\n",
    "        b.h = self.emb(b.h)\n",
    "        \n",
    "        b = self.gnn(b)\n",
    "        h_nodes = self.pre_mlp(b.h)\n",
    "        \n",
    "        # h_graph is num_graphs x hid_dim\n",
    "        h_graph = index_sum(b.nG, h_nodes, b.batch, self.cuda) \n",
    "        \n",
    "        out = self.post_mlp(h_graph)\n",
    "        return out"
   ],
   "id": "10aad7c4"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "b7f4cef6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "cuda = True\n",
    "\n",
    "model = NetEGNN(n_layers=7, cuda=cuda)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-16)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, verbose=False)"
   ],
   "id": "b7f4cef6"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e5d6b1c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training"
   ],
   "id": "4e5d6b1c"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "de3613c9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "8baf99c0-04cc-4e9b-accb-81963362745b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> start training\n",
      "> epoch 000: train 347.839 val 292.940 test 288.890 (115.6 sec)\n",
      "> epoch 001: train 236.499 val 215.980 test 215.799 (108.8 sec)\n",
      "> epoch 002: train 189.462 val 177.226 test 177.115 (111.3 sec)\n",
      "> epoch 003: train 166.638 val 142.420 test 140.712 (109.4 sec)\n",
      "> epoch 004: train 151.305 val 140.691 test 139.867 (110.3 sec)\n",
      "> epoch 005: train 140.757 val 137.295 test 137.778 (111.0 sec)\n",
      "> epoch 006: train 133.721 val 129.073 test 129.840 (110.2 sec)\n",
      "> epoch 007: train 125.512 val 114.223 test 113.974 (110.7 sec)\n",
      "> epoch 008: train 119.024 val 113.256 test 111.740 (108.9 sec)\n",
      "> epoch 009: train 114.211 val 105.659 test 105.743 (108.5 sec)\n",
      "> epoch 010: train 111.649 val 100.730 test 101.223 (107.4 sec)\n",
      "> epoch 011: train 106.646 val 99.395 test 99.002 (107.3 sec)\n",
      "> epoch 012: train 104.310 val 97.588 test 97.101 (107.3 sec)\n",
      "> epoch 013: train 101.760 val 95.613 test 94.903 (107.2 sec)\n",
      "> epoch 014: train 99.546 val 93.259 test 93.540 (107.4 sec)\n",
      "> epoch 015: train 97.121 val 90.587 test 90.449 (108.3 sec)\n",
      "> epoch 016: train 95.254 val 89.234 test 89.609 (107.6 sec)\n",
      "> epoch 017: train 93.918 val 87.375 test 87.541 (106.3 sec)\n",
      "> epoch 018: train 91.891 val 86.680 test 86.987 (107.3 sec)\n",
      "> epoch 019: train 91.117 val 94.118 test 95.755 (107.6 sec)\n",
      "> epoch 020: train 89.292 val 87.126 test 87.137 (107.4 sec)\n",
      "> epoch 021: train 87.362 val 82.831 test 82.289 (107.6 sec)\n",
      "> epoch 022: train 85.925 val 81.438 test 82.010 (107.8 sec)\n",
      "> epoch 023: train 85.346 val 82.954 test 83.805 (108.6 sec)\n",
      "> epoch 024: train 84.285 val 77.116 test 77.915 (108.7 sec)\n",
      "> epoch 025: train 83.418 val 79.380 test 79.778 (108.0 sec)\n",
      "> epoch 026: train 82.371 val 78.929 test 78.708 (108.7 sec)\n",
      "> epoch 027: train 80.740 val 76.983 test 77.806 (108.8 sec)\n",
      "> epoch 028: train 80.249 val 78.374 test 78.671 (108.8 sec)\n",
      "> epoch 029: train 79.053 val 70.718 test 70.882 (108.8 sec)\n",
      "> epoch 030: train 79.026 val 71.257 test 72.217 (108.9 sec)\n",
      "> epoch 031: train 77.883 val 72.139 test 72.822 (108.2 sec)\n",
      "> epoch 032: train 76.906 val 72.380 test 72.652 (107.2 sec)\n",
      "> epoch 033: train 75.925 val 69.847 test 70.456 (106.8 sec)\n",
      "> epoch 034: train 75.607 val 69.318 test 68.558 (106.7 sec)\n",
      "> epoch 035: train 74.852 val 74.883 test 74.814 (107.1 sec)\n",
      "> epoch 036: train 74.379 val 72.076 test 73.537 (106.8 sec)\n",
      "> epoch 037: train 74.071 val 71.617 test 72.403 (107.5 sec)\n",
      "> epoch 038: train 73.764 val 67.722 test 69.409 (107.5 sec)\n",
      "> epoch 039: train 72.999 val 71.654 test 71.866 (107.3 sec)\n",
      "> epoch 040: train 71.643 val 79.038 test 79.165 (106.1 sec)\n",
      "> epoch 041: train 72.018 val 71.355 test 71.633 (108.5 sec)\n",
      "> epoch 042: train 71.193 val 64.515 test 65.274 (108.6 sec)\n",
      "> epoch 043: train 72.625 val 77.266 test 77.959 (107.0 sec)\n",
      "> epoch 044: train 70.566 val 67.575 test 68.218 (108.0 sec)\n",
      "> epoch 045: train 69.238 val 65.836 test 65.894 (108.8 sec)\n",
      "> epoch 046: train 69.325 val 64.543 test 65.363 (108.8 sec)\n",
      "> epoch 047: train 68.555 val 69.625 test 69.585 (108.0 sec)\n",
      "> epoch 048: train 68.866 val 64.446 test 65.418 (106.3 sec)\n",
      "> epoch 049: train 68.210 val 67.437 test 67.522 (107.5 sec)\n",
      "> epoch 050: train 67.821 val 62.315 test 63.184 (108.2 sec)\n",
      "> epoch 051: train 67.153 val 65.458 test 66.239 (107.3 sec)\n",
      "> epoch 052: train 66.726 val 65.827 test 66.801 (107.5 sec)\n",
      "> epoch 053: train 66.455 val 60.760 test 60.988 (107.6 sec)\n",
      "> epoch 054: train 66.000 val 63.312 test 64.087 (106.6 sec)\n",
      "> epoch 055: train 65.189 val 67.651 test 68.241 (108.7 sec)\n",
      "> epoch 056: train 65.327 val 63.785 test 65.083 (107.4 sec)\n",
      "> epoch 057: train 64.863 val 61.946 test 63.046 (107.4 sec)\n",
      "> epoch 058: train 63.954 val 61.347 test 61.341 (107.4 sec)\n",
      "> epoch 059: train 64.567 val 61.114 test 62.471 (106.7 sec)\n",
      "> epoch 060: train 63.328 val 63.956 test 64.914 (107.5 sec)\n",
      "> epoch 061: train 63.321 val 59.468 test 60.147 (107.4 sec)\n",
      "> epoch 062: train 63.755 val 64.056 test 64.490 (107.6 sec)\n",
      "> epoch 063: train 63.044 val 60.421 test 60.060 (108.8 sec)\n",
      "> epoch 064: train 62.083 val 59.820 test 59.709 (108.2 sec)\n",
      "> epoch 065: train 62.454 val 66.483 test 66.167 (107.2 sec)\n",
      "> epoch 066: train 61.787 val 57.973 test 59.033 (107.2 sec)\n",
      "> epoch 067: train 61.687 val 61.050 test 61.963 (108.4 sec)\n",
      "> epoch 068: train 61.144 val 63.132 test 63.890 (106.8 sec)\n",
      "> epoch 069: train 60.677 val 57.872 test 59.084 (106.7 sec)\n",
      "> epoch 070: train 60.459 val 58.451 test 58.985 (107.3 sec)\n",
      "> epoch 071: train 60.525 val 58.294 test 57.959 (107.2 sec)\n",
      "> epoch 072: train 59.877 val 59.296 test 59.172 (107.6 sec)\n",
      "> epoch 073: train 60.335 val 58.661 test 59.594 (107.4 sec)\n",
      "> epoch 074: train 59.754 val 56.703 test 56.350 (107.5 sec)\n",
      "> epoch 075: train 58.937 val 59.272 test 59.653 (107.1 sec)\n",
      "> epoch 076: train 59.000 val 57.677 test 57.883 (107.2 sec)\n",
      "> epoch 077: train 59.313 val 61.700 test 62.216 (107.4 sec)\n",
      "> epoch 078: train 58.932 val 57.545 test 58.168 (107.5 sec)\n",
      "> epoch 079: train 58.696 val 57.900 test 58.637 (107.1 sec)\n",
      "> epoch 080: train 57.765 val 59.081 test 58.984 (107.7 sec)\n",
      "> epoch 081: train 58.104 val 57.384 test 56.940 (107.8 sec)\n",
      "> epoch 082: train 58.024 val 59.605 test 60.355 (106.9 sec)\n",
      "> epoch 083: train 57.078 val 56.087 test 56.895 (107.6 sec)\n",
      "> epoch 084: train 57.481 val 57.166 test 57.559 (106.9 sec)\n",
      "> epoch 085: train 57.134 val 56.476 test 56.808 (107.3 sec)\n",
      "> epoch 086: train 57.022 val 57.628 test 58.010 (107.2 sec)\n",
      "> epoch 087: train 56.512 val 58.755 test 59.261 (108.0 sec)\n",
      "> epoch 088: train 56.410 val 65.248 test 64.812 (107.4 sec)\n",
      "> epoch 089: train 55.753 val 55.246 test 55.842 (107.1 sec)\n",
      "> epoch 090: train 56.112 val 56.141 test 56.792 (107.4 sec)\n",
      "> epoch 091: train 55.974 val 58.685 test 58.216 (107.3 sec)\n",
      "> epoch 092: train 55.571 val 55.065 test 56.007 (107.8 sec)\n",
      "> epoch 093: train 55.499 val 57.881 test 56.883 (107.4 sec)\n",
      "> epoch 094: train 55.418 val 54.603 test 55.147 (107.5 sec)\n",
      "> epoch 095: train 55.242 val 53.232 test 53.795 (107.1 sec)\n",
      "> epoch 096: train 55.055 val 55.285 test 55.431 (107.2 sec)\n",
      "> epoch 097: train 54.928 val 56.313 test 56.911 (108.4 sec)\n",
      "> epoch 098: train 54.800 val 54.029 test 54.712 (107.6 sec)\n",
      "> epoch 099: train 54.605 val 52.156 test 53.117 (106.7 sec)\n",
      "> epoch 100: train 54.193 val 55.949 test 56.038 (107.2 sec)\n",
      "> epoch 101: train 54.341 val 53.205 test 53.606 (107.6 sec)\n",
      "> epoch 102: train 54.281 val 55.575 test 55.813 (107.4 sec)\n",
      "> epoch 103: train 53.857 val 54.037 test 54.612 (107.2 sec)\n",
      "> epoch 104: train 53.817 val 53.875 test 54.997 (107.6 sec)\n",
      "> epoch 105: train 53.021 val 58.040 test 58.340 (107.5 sec)\n",
      "> epoch 106: train 53.538 val 53.264 test 54.171 (107.2 sec)\n",
      "> epoch 107: train 53.203 val 53.127 test 54.088 (107.0 sec)\n",
      "> epoch 108: train 52.792 val 53.908 test 54.715 (107.3 sec)\n",
      "> epoch 109: train 52.611 val 55.660 test 56.152 (107.8 sec)\n",
      "> epoch 110: train 52.760 val 55.159 test 55.527 (108.5 sec)\n",
      "> epoch 111: train 52.726 val 56.550 test 57.089 (107.6 sec)\n",
      "> epoch 112: train 52.946 val 54.745 test 55.885 (107.4 sec)\n",
      "> epoch 113: train 52.272 val 50.475 test 51.074 (107.2 sec)\n",
      "> epoch 114: train 51.707 val 51.764 test 52.434 (108.8 sec)\n",
      "> epoch 115: train 52.129 val 55.365 test 55.743 (108.2 sec)\n",
      "> epoch 116: train 52.194 val 56.333 test 56.830 (108.0 sec)\n",
      "> epoch 117: train 51.734 val 52.787 test 53.307 (108.4 sec)\n",
      "> epoch 118: train 51.408 val 57.486 test 58.299 (108.4 sec)\n",
      "> epoch 119: train 52.129 val 54.292 test 55.412 (108.1 sec)\n",
      "> epoch 120: train 51.596 val 51.389 test 51.999 (106.5 sec)\n",
      "> epoch 121: train 51.477 val 49.522 test 50.214 (107.7 sec)\n",
      "> epoch 122: train 50.948 val 53.125 test 54.267 (107.5 sec)\n",
      "> epoch 123: train 51.059 val 52.401 test 52.466 (108.0 sec)\n",
      "> epoch 124: train 50.686 val 55.481 test 56.373 (108.6 sec)\n",
      "> epoch 125: train 50.725 val 53.556 test 54.470 (108.8 sec)\n",
      "> epoch 126: train 51.072 val 54.469 test 54.840 (107.8 sec)\n",
      "> epoch 127: train 50.307 val 51.001 test 51.679 (107.6 sec)\n",
      "> epoch 128: train 50.115 val 51.608 test 52.618 (106.8 sec)\n",
      "> epoch 129: train 50.081 val 55.073 test 54.942 (107.5 sec)\n",
      "> epoch 130: train 50.129 val 54.473 test 54.449 (107.9 sec)\n",
      "> epoch 131: train 49.817 val 53.754 test 54.556 (107.6 sec)\n",
      "> epoch 132: train 49.606 val 54.045 test 55.078 (107.5 sec)\n",
      "> epoch 133: train 49.792 val 51.565 test 53.038 (107.6 sec)\n",
      "> epoch 134: train 49.866 val 50.424 test 50.701 (107.4 sec)\n",
      "> epoch 135: train 49.543 val 50.685 test 51.559 (107.5 sec)\n",
      "> epoch 136: train 49.500 val 53.363 test 53.406 (107.2 sec)\n",
      "> epoch 137: train 49.086 val 53.485 test 53.490 (108.7 sec)\n",
      "> epoch 138: train 49.131 val 50.231 test 51.217 (108.8 sec)\n",
      "> epoch 139: train 49.256 val 50.855 test 51.086 (108.8 sec)\n",
      "> epoch 140: train 48.788 val 50.901 test 51.713 (108.8 sec)\n",
      "> epoch 141: train 49.044 val 48.243 test 48.773 (108.7 sec)\n",
      "> epoch 142: train 48.751 val 50.703 test 51.365 (106.5 sec)\n",
      "> epoch 143: train 48.528 val 50.489 test 50.476 (106.8 sec)\n",
      "> epoch 144: train 48.584 val 49.545 test 50.939 (107.6 sec)\n",
      "> epoch 145: train 49.100 val 50.767 test 51.452 (107.4 sec)\n",
      "> epoch 146: train 48.969 val 52.287 test 53.183 (107.1 sec)\n",
      "> epoch 147: train 48.066 val 51.174 test 51.716 (107.5 sec)\n",
      "> epoch 148: train 47.999 val 51.196 test 52.545 (108.8 sec)\n",
      "> epoch 149: train 47.637 val 50.690 test 51.166 (106.7 sec)\n",
      "> epoch 150: train 47.807 val 49.397 test 50.380 (107.9 sec)\n",
      "> epoch 151: train 47.839 val 48.427 test 49.498 (106.9 sec)\n",
      "> epoch 152: train 47.735 val 51.376 test 52.030 (106.5 sec)\n",
      "> epoch 153: train 47.295 val 48.053 test 48.175 (107.5 sec)\n",
      "> epoch 154: train 47.270 val 49.496 test 49.798 (107.4 sec)\n",
      "> epoch 155: train 47.363 val 51.323 test 52.170 (107.8 sec)\n",
      "> epoch 156: train 47.653 val 52.207 test 52.522 (106.8 sec)\n",
      "> epoch 157: train 47.506 val 50.987 test 52.070 (107.1 sec)\n",
      "> epoch 158: train 47.190 val 49.777 test 50.857 (107.5 sec)\n",
      "> epoch 159: train 47.144 val 50.088 test 50.627 (106.7 sec)\n",
      "> epoch 160: train 46.946 val 53.586 test 53.533 (108.7 sec)\n",
      "> epoch 161: train 46.647 val 50.874 test 50.855 (108.8 sec)\n",
      "> epoch 162: train 46.879 val 53.458 test 53.304 (108.9 sec)\n",
      "> epoch 163: train 46.804 val 48.191 test 48.176 (107.1 sec)\n",
      "> epoch 164: train 46.366 val 49.018 test 49.967 (107.5 sec)\n",
      "> epoch 165: train 46.619 val 48.217 test 49.555 (106.4 sec)\n",
      "> epoch 166: train 46.255 val 49.424 test 49.806 (107.4 sec)\n",
      "> epoch 167: train 46.797 val 49.602 test 50.163 (107.2 sec)\n",
      "> epoch 168: train 45.955 val 47.909 test 48.107 (107.5 sec)\n",
      "> epoch 169: train 46.415 val 51.753 test 52.377 (107.1 sec)\n",
      "> epoch 170: train 46.919 val 48.052 test 48.338 (107.3 sec)\n",
      "> epoch 171: train 45.870 val 48.342 test 48.941 (107.4 sec)\n",
      "> epoch 172: train 45.969 val 48.132 test 48.154 (106.6 sec)\n",
      "> epoch 173: train 46.065 val 48.523 test 48.930 (107.9 sec)\n",
      "> epoch 174: train 46.228 val 49.265 test 50.088 (108.7 sec)\n",
      "> epoch 175: train 45.621 val 48.296 test 48.581 (107.5 sec)\n",
      "> epoch 176: train 45.707 val 50.566 test 51.435 (107.5 sec)\n",
      "> epoch 177: train 45.480 val 51.775 test 51.641 (107.5 sec)\n",
      "> epoch 178: train 45.552 val 47.132 test 47.561 (107.0 sec)\n",
      "> epoch 179: train 45.558 val 47.812 test 48.314 (107.6 sec)\n",
      "> epoch 180: train 45.497 val 47.205 test 47.478 (107.5 sec)\n",
      "> epoch 181: train 45.100 val 49.368 test 50.627 (107.7 sec)\n",
      "> epoch 182: train 45.347 val 50.462 test 50.301 (107.4 sec)\n",
      "> epoch 183: train 45.280 val 48.632 test 48.910 (108.2 sec)\n",
      "> epoch 184: train 45.135 val 51.213 test 51.429 (107.6 sec)\n",
      "> epoch 185: train 45.064 val 47.733 test 48.411 (107.0 sec)\n",
      "> epoch 186: train 45.173 val 48.297 test 48.773 (106.9 sec)\n",
      "> epoch 187: train 44.905 val 49.119 test 49.877 (107.5 sec)\n",
      "> epoch 188: train 44.566 val 53.182 test 53.888 (107.6 sec)\n",
      "> epoch 189: train 44.952 val 49.202 test 49.580 (107.3 sec)\n",
      "> epoch 190: train 44.338 val 47.323 test 47.915 (106.6 sec)\n",
      "> epoch 191: train 44.954 val 47.523 test 47.703 (107.0 sec)\n",
      "> epoch 192: train 44.242 val 48.243 test 48.614 (107.1 sec)\n",
      "> epoch 193: train 44.197 val 46.796 test 47.152 (107.6 sec)\n",
      "> epoch 194: train 44.195 val 46.494 test 47.197 (108.1 sec)\n",
      "> epoch 195: train 44.024 val 47.319 test 48.443 (108.7 sec)\n",
      "> epoch 196: train 44.421 val 52.549 test 53.084 (107.5 sec)\n",
      "> epoch 197: train 44.328 val 47.381 test 47.464 (107.1 sec)\n",
      "> epoch 198: train 43.893 val 50.099 test 50.104 (107.6 sec)\n",
      "> epoch 199: train 43.690 val 48.601 test 49.073 (108.1 sec)\n",
      "> epoch 200: train 43.686 val 46.883 test 48.815 (108.7 sec)\n",
      "> epoch 201: train 43.590 val 51.859 test 52.808 (108.9 sec)\n",
      "> epoch 202: train 43.839 val 50.133 test 50.771 (108.8 sec)\n",
      "> epoch 203: train 43.512 val 49.289 test 50.096 (108.1 sec)\n",
      "> epoch 204: train 43.627 val 50.262 test 51.164 (107.0 sec)\n",
      "> epoch 205: train 43.678 val 45.521 test 46.249 (107.5 sec)\n",
      "> epoch 206: train 43.042 val 45.797 test 46.220 (107.7 sec)\n",
      "> epoch 207: train 42.980 val 48.447 test 48.663 (107.6 sec)\n",
      "> epoch 208: train 43.177 val 50.294 test 50.891 (107.4 sec)\n",
      "> epoch 209: train 43.167 val 46.813 test 48.492 (107.3 sec)\n",
      "> epoch 210: train 43.545 val 49.221 test 49.626 (107.5 sec)\n",
      "> epoch 211: train 42.958 val 45.427 test 45.701 (107.5 sec)\n",
      "> epoch 212: train 42.602 val 45.069 test 45.562 (107.6 sec)\n",
      "> epoch 213: train 43.226 val 48.057 test 48.554 (107.5 sec)\n",
      "> epoch 214: train 42.602 val 45.857 test 45.971 (107.4 sec)\n",
      "> epoch 215: train 42.684 val 46.066 test 45.873 (107.6 sec)\n",
      "> epoch 216: train 42.686 val 49.960 test 50.221 (107.5 sec)\n",
      "> epoch 217: train 42.661 val 47.389 test 47.886 (107.8 sec)\n",
      "> epoch 218: train 42.526 val 46.053 test 46.210 (107.5 sec)\n",
      "> epoch 219: train 43.061 val 48.134 test 49.063 (107.5 sec)\n",
      "> epoch 220: train 42.035 val 45.100 test 45.723 (108.8 sec)\n",
      "> epoch 221: train 42.265 val 45.609 test 46.583 (108.7 sec)\n",
      "> epoch 222: train 42.403 val 47.380 test 48.819 (108.8 sec)\n",
      "> epoch 223: train 42.267 val 48.514 test 49.289 (108.2 sec)\n",
      "> epoch 224: train 41.737 val 46.052 test 47.209 (107.5 sec)\n",
      "> epoch 225: train 42.206 val 49.704 test 50.640 (107.6 sec)\n",
      "> epoch 226: train 41.948 val 46.576 test 46.844 (107.4 sec)\n",
      "> epoch 227: train 42.210 val 46.448 test 46.651 (107.6 sec)\n",
      "> epoch 228: train 41.902 val 48.574 test 49.159 (107.9 sec)\n",
      "> epoch 229: train 41.539 val 44.442 test 45.139 (108.7 sec)\n",
      "> epoch 230: train 41.650 val 45.633 test 45.930 (108.8 sec)\n",
      "> epoch 231: train 41.629 val 44.923 test 44.887 (108.8 sec)\n",
      "> epoch 232: train 41.503 val 45.546 test 45.674 (107.7 sec)\n",
      "> epoch 233: train 41.628 val 45.050 test 45.190 (107.5 sec)\n",
      "> epoch 234: train 41.566 val 47.026 test 47.254 (107.5 sec)\n",
      "> epoch 235: train 41.525 val 45.953 test 46.232 (107.0 sec)\n",
      "> epoch 236: train 41.135 val 46.131 test 46.280 (107.5 sec)\n",
      "> epoch 237: train 41.332 val 46.456 test 46.968 (107.9 sec)\n",
      "> epoch 238: train 41.235 val 44.618 test 45.156 (107.1 sec)\n",
      "> epoch 239: train 40.808 val 46.009 test 46.706 (107.7 sec)\n",
      "> epoch 240: train 41.085 val 44.198 test 44.971 (108.8 sec)\n",
      "> epoch 241: train 40.682 val 45.797 test 46.624 (108.8 sec)\n",
      "> epoch 242: train 41.056 val 47.220 test 47.755 (108.7 sec)\n",
      "> epoch 243: train 41.028 val 46.956 test 47.366 (108.7 sec)\n",
      "> epoch 244: train 40.879 val 47.214 test 48.050 (108.8 sec)\n",
      "> epoch 245: train 40.432 val 46.987 test 47.826 (108.7 sec)\n",
      "> epoch 246: train 40.923 val 48.580 test 48.852 (106.0 sec)\n",
      "> epoch 247: train 40.501 val 44.474 test 44.751 (107.5 sec)\n",
      "> epoch 248: train 40.834 val 45.378 test 45.794 (106.8 sec)\n",
      "> epoch 249: train 40.254 val 46.896 test 47.811 (107.5 sec)\n",
      "> epoch 250: train 40.734 val 46.137 test 46.146 (108.4 sec)\n",
      "> epoch 251: train 40.218 val 44.570 test 45.523 (108.6 sec)\n",
      "> epoch 252: train 39.923 val 44.852 test 44.873 (108.1 sec)\n",
      "> epoch 253: train 40.415 val 46.642 test 46.973 (107.6 sec)\n",
      "> epoch 254: train 40.196 val 46.548 test 47.635 (107.5 sec)\n",
      "> epoch 255: train 39.663 val 46.155 test 46.934 (107.1 sec)\n",
      "> epoch 256: train 40.375 val 46.338 test 46.655 (108.9 sec)\n",
      "> epoch 257: train 39.816 val 46.443 test 47.474 (106.4 sec)\n",
      "> epoch 258: train 39.881 val 46.002 test 45.758 (107.1 sec)\n",
      "> epoch 259: train 40.255 val 47.790 test 48.283 (107.2 sec)\n",
      "> epoch 260: train 39.420 val 51.888 test 52.710 (108.7 sec)\n",
      "> epoch 261: train 39.569 val 46.663 test 47.706 (107.2 sec)\n",
      "> epoch 262: train 40.045 val 45.518 test 45.942 (107.7 sec)\n",
      "> epoch 263: train 39.556 val 45.898 test 46.411 (108.8 sec)\n",
      "> epoch 264: train 39.666 val 45.773 test 46.738 (108.9 sec)\n",
      "> epoch 265: train 39.104 val 45.582 test 46.588 (108.2 sec)\n",
      "> epoch 266: train 39.494 val 47.091 test 47.207 (107.7 sec)\n",
      "> epoch 267: train 39.722 val 44.096 test 44.605 (107.9 sec)\n",
      "> epoch 268: train 39.433 val 47.640 test 48.512 (107.7 sec)\n",
      "> epoch 269: train 39.584 val 46.905 test 47.541 (106.7 sec)\n",
      "> epoch 270: train 38.873 val 46.840 test 47.340 (107.6 sec)\n",
      "> epoch 271: train 38.952 val 45.028 test 45.547 (107.3 sec)\n",
      "> epoch 272: train 38.872 val 46.078 test 46.382 (108.4 sec)\n",
      "> epoch 273: train 39.136 val 48.310 test 48.726 (108.0 sec)\n",
      "> epoch 274: train 39.025 val 43.997 test 44.572 (107.0 sec)\n",
      "> epoch 275: train 38.785 val 43.504 test 44.194 (107.6 sec)\n",
      "> epoch 276: train 38.902 val 43.532 test 44.210 (107.3 sec)\n",
      "> epoch 277: train 38.653 val 51.557 test 51.238 (107.5 sec)\n",
      "> epoch 278: train 39.020 val 47.129 test 47.628 (107.6 sec)\n",
      "> epoch 279: train 39.013 val 43.879 test 44.264 (107.6 sec)\n",
      "> epoch 280: train 38.413 val 42.188 test 42.738 (107.6 sec)\n",
      "> epoch 281: train 38.505 val 44.981 test 45.737 (107.2 sec)\n",
      "> epoch 282: train 38.201 val 51.536 test 51.337 (107.3 sec)\n",
      "> epoch 283: train 38.014 val 52.000 test 52.238 (107.5 sec)\n",
      "> epoch 284: train 42.290 val 44.883 test 45.617 (107.6 sec)\n",
      "> epoch 285: train 38.534 val 44.548 test 45.039 (107.5 sec)\n",
      "> epoch 286: train 37.766 val 48.636 test 48.905 (107.5 sec)\n",
      "> epoch 287: train 37.742 val 46.867 test 47.422 (107.3 sec)\n",
      "> epoch 288: train 37.976 val 46.367 test 46.183 (107.9 sec)\n",
      "> epoch 289: train 37.851 val 46.412 test 46.504 (106.5 sec)\n",
      "> epoch 290: train 37.893 val 47.697 test 47.893 (107.3 sec)\n",
      "> epoch 291: train 37.772 val 47.097 test 47.939 (107.6 sec)\n",
      "> epoch 292: train 37.657 val 44.984 test 46.047 (107.2 sec)\n",
      "> epoch 293: train 37.744 val 46.638 test 46.587 (108.2 sec)\n",
      "> epoch 294: train 37.596 val 47.680 test 47.869 (108.8 sec)\n",
      "> epoch 295: train 37.638 val 44.766 test 45.595 (108.8 sec)\n",
      "> epoch 296: train 37.312 val 48.945 test 49.279 (108.8 sec)\n",
      "> epoch 297: train 37.154 val 47.807 test 48.123 (108.8 sec)\n",
      "> epoch 298: train 37.078 val 53.577 test 53.743 (108.8 sec)\n",
      "> epoch 299: train 37.100 val 48.511 test 48.883 (108.8 sec)\n",
      "> epoch 300: train 36.990 val 55.370 test 55.570 (107.7 sec)\n",
      "> epoch 301: train 36.810 val 50.596 test 50.793 (107.5 sec)\n",
      "> epoch 302: train 36.970 val 46.297 test 47.008 (107.4 sec)\n",
      "> epoch 303: train 36.625 val 46.836 test 47.194 (107.6 sec)\n",
      "> epoch 304: train 36.675 val 49.991 test 50.565 (107.6 sec)\n",
      "> epoch 305: train 36.563 val 56.691 test 56.582 (106.8 sec)\n",
      "> epoch 306: train 36.436 val 47.272 test 47.392 (107.7 sec)\n",
      "> epoch 307: train 36.141 val 52.306 test 53.317 (108.8 sec)\n",
      "> epoch 308: train 36.529 val 52.943 test 52.971 (107.7 sec)\n",
      "> epoch 309: train 36.305 val 63.897 test 64.021 (107.1 sec)\n",
      "> epoch 310: train 35.737 val 57.995 test 58.169 (106.9 sec)\n",
      "> epoch 311: train 35.788 val 60.649 test 60.833 (107.6 sec)\n",
      "> epoch 312: train 35.722 val 60.659 test 61.357 (107.5 sec)\n",
      "> epoch 313: train 35.070 val 62.946 test 62.825 (106.9 sec)\n",
      "> epoch 314: train 35.463 val 66.837 test 66.956 (107.6 sec)\n",
      "> epoch 315: train 35.196 val 66.943 test 66.787 (107.7 sec)\n",
      "> epoch 316: train 35.116 val 55.631 test 56.442 (108.7 sec)\n",
      "> epoch 317: train 35.136 val 64.702 test 64.598 (108.8 sec)\n",
      "> epoch 318: train 35.145 val 65.441 test 65.320 (108.8 sec)\n",
      "> epoch 319: train 34.825 val 69.559 test 70.138 (108.7 sec)\n",
      "> epoch 320: train 34.420 val 72.171 test 72.216 (108.9 sec)\n",
      "> epoch 321: train 34.423 val 78.608 test 78.567 (108.8 sec)\n",
      "> epoch 322: train 34.284 val 78.791 test 79.034 (109.0 sec)\n",
      "> epoch 323: train 34.468 val 79.121 test 79.114 (108.4 sec)\n",
      "> epoch 324: train 34.275 val 84.775 test 84.486 (107.6 sec)\n",
      "> epoch 325: train 33.776 val 84.491 test 84.663 (106.9 sec)\n",
      "> epoch 326: train 33.696 val 88.320 test 88.131 (107.6 sec)\n",
      "> epoch 327: train 33.863 val 81.950 test 82.314 (107.6 sec)\n",
      "> epoch 328: train 33.339 val 93.932 test 94.472 (107.1 sec)\n",
      "> epoch 329: train 32.990 val 100.739 test 100.519 (107.4 sec)\n",
      "> epoch 330: train 32.931 val 95.904 test 96.032 (107.6 sec)\n",
      "> epoch 331: train 33.037 val 93.915 test 94.279 (107.3 sec)\n",
      "> epoch 332: train 32.769 val 102.209 test 102.328 (107.6 sec)\n",
      "> epoch 333: train 32.919 val 101.763 test 101.663 (107.3 sec)\n",
      "> epoch 334: train 32.870 val 100.426 test 100.194 (108.8 sec)\n",
      "> epoch 335: train 32.064 val 119.452 test 119.148 (107.1 sec)\n",
      "> epoch 336: train 32.194 val 110.633 test 110.609 (107.4 sec)\n",
      "> epoch 337: train 32.059 val 117.084 test 116.569 (107.6 sec)\n",
      "> epoch 338: train 31.916 val 119.834 test 119.424 (107.6 sec)\n",
      "> epoch 339: train 31.700 val 107.219 test 107.012 (106.8 sec)\n",
      "> epoch 340: train 31.757 val 123.066 test 122.502 (108.4 sec)\n",
      "> epoch 341: train 31.293 val 112.483 test 112.182 (108.5 sec)\n",
      "> epoch 342: train 31.465 val 126.370 test 125.415 (107.7 sec)\n",
      "> epoch 343: train 31.103 val 128.019 test 127.012 (106.9 sec)\n",
      "> epoch 344: train 30.911 val 122.580 test 122.566 (107.6 sec)\n",
      "> epoch 345: train 31.002 val 137.515 test 136.638 (107.6 sec)\n",
      "> epoch 346: train 30.689 val 135.295 test 134.467 (106.3 sec)\n",
      "> epoch 347: train 30.403 val 135.066 test 134.759 (107.2 sec)\n",
      "> epoch 348: train 30.645 val 136.682 test 135.332 (106.5 sec)\n",
      "> epoch 349: train 67.980 val 142.201 test 141.687 (106.6 sec)\n",
      "> epoch 350: train 33.311 val 135.079 test 134.399 (108.4 sec)\n",
      "> epoch 351: train 29.536 val 135.809 test 135.196 (107.6 sec)\n",
      "> epoch 352: train 28.199 val 135.169 test 134.926 (107.6 sec)\n",
      "> epoch 353: train 28.275 val 134.632 test 134.331 (106.8 sec)\n",
      "> epoch 354: train 27.641 val 152.411 test 151.581 (107.4 sec)\n",
      "> epoch 355: train 27.453 val 145.113 test 144.711 (107.5 sec)\n",
      "> epoch 356: train 28.014 val 148.520 test 147.933 (107.5 sec)\n",
      "> epoch 357: train 28.184 val 148.232 test 147.586 (107.7 sec)\n",
      "> epoch 358: train 28.195 val 155.425 test 154.995 (107.6 sec)\n",
      "> epoch 359: train 28.796 val 146.036 test 146.013 (107.4 sec)\n",
      "> epoch 360: train 28.765 val 161.738 test 160.701 (107.3 sec)\n",
      "> epoch 361: train 28.973 val 158.807 test 157.844 (108.4 sec)\n",
      "> epoch 362: train 28.867 val 158.517 test 157.595 (108.0 sec)\n",
      "> epoch 363: train 28.484 val 165.562 test 164.562 (108.3 sec)\n",
      "> epoch 364: train 28.612 val 166.409 test 165.344 (108.4 sec)\n",
      "> epoch 365: train 28.955 val 170.006 test 168.876 (107.2 sec)\n",
      "> epoch 366: train 28.419 val 170.102 test 168.966 (107.5 sec)\n",
      "> epoch 367: train 28.453 val 170.601 test 169.387 (107.6 sec)\n",
      "> epoch 368: train 28.265 val 167.320 test 166.112 (107.5 sec)\n",
      "> epoch 369: train 27.862 val 170.618 test 169.330 (107.8 sec)\n",
      "> epoch 370: train 28.090 val 174.155 test 172.837 (108.4 sec)\n",
      "> epoch 371: "
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [37]\u001B[0m, in \u001B[0;36m<cell line: 14>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m     35\u001B[0m         loss \u001B[38;5;241m=\u001B[39m  F\u001B[38;5;241m.\u001B[39ml1_loss(out\u001B[38;5;241m*\u001B[39mmad\u001B[38;5;241m+\u001B[39mme, batch\u001B[38;5;241m.\u001B[39my)\n\u001B[1;32m---> 37\u001B[0m     batch_train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mfloat\u001B[39m(\u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mnumpy())]  \n\u001B[0;32m     39\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mmean(batch_train_loss)\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m0.001\u001B[39m]\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain \u001B[39m\u001B[38;5;132;01m%.3f\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m train_loss[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m, flush\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print('> start training')\n",
    "\n",
    "tr_ys = train_loader.dataset.data['homo'] \n",
    "me, mad = torch.mean(tr_ys), torch.mean(torch.abs(tr_ys - torch.mean(tr_ys)))\n",
    "\n",
    "if cuda:\n",
    "    me = me.cuda()\n",
    "    mad = mad.cuda()\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('> epoch %s:' % str(epoch).zfill(3), end=' ', flush=True) \n",
    "    start = time.time()\n",
    "\n",
    "    batch_train_loss = []\n",
    "    batch_val_loss = []\n",
    "    batch_test_loss = []\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = BatchGraph(batch, cuda, charge_scale)\n",
    "        \n",
    "        out = model(batch).reshape(-1)\n",
    "        \n",
    "        loss =  F.l1_loss(out, (batch.y - me) / mad)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss =  F.l1_loss(out*mad+me, batch.y)\n",
    "\n",
    "        batch_train_loss += [float(loss.data.cpu().numpy())]  \n",
    "        \n",
    "    train_loss += [np.mean(batch_train_loss)/0.001]\n",
    "    \n",
    "    print('train %.3f' % train_loss[-1], end=' ', flush=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in val_loader:\n",
    "            batch = BatchGraph(batch, cuda, charge_scale)\n",
    "            out = model(batch).reshape(-1)\n",
    "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
    "            batch_val_loss += [np.mean(loss)]\n",
    "            \n",
    "        val_loss += [np.mean(batch_val_loss)/0.001]\n",
    "        \n",
    "        print('val %.3f' % val_loss[-1], end=' ', flush=True)\n",
    "        \n",
    "        for batch in test_loader:\n",
    "            batch = BatchGraph(batch, cuda, charge_scale)\n",
    "            out = model(batch).reshape(-1)\n",
    "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
    "            batch_test_loss += [np.mean(loss)]\n",
    "\n",
    "        test_loss += [np.mean(batch_test_loss)/0.001]\n",
    "        \n",
    "    end = time.time()\n",
    "\n",
    "    print('test %.3f (%.1f sec)' % (test_loss[-1], end-start), flush=True)\n",
    "    lr_scheduler.step()"
   ],
   "id": "de3613c9"
  },
  {
   "cell_type": "code",
   "source": [
    "# > epoch 280: train 38.413 val 42.188 test 42.738 (107.6 sec)\n",
    "# Дальше модель начала переобучаться"
   ],
   "metadata": {
    "id": "qr24yxTIEQ9H",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "qr24yxTIEQ9H",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7825a8f6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This is good results \n",
    "# Lower is better\n",
    "# val 30.157 test 30.886"
   ],
   "id": "7825a8f6"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Копия блокнота \"graph_seminar_homework.ipynb\"",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}